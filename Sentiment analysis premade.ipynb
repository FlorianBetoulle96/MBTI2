{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cacf188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\fbeto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0a7b32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>line_text</th>\n",
       "      <th>MBTI</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andy</td>\n",
       "      <td>Hey, Big Tuna!  You're single right? She's pre...</td>\n",
       "      <td>ESFP</td>\n",
       "      <td>Se</td>\n",
       "      <td>Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angela</td>\n",
       "      <td>I bet it's gonna be me. Probably gonna be me. ...</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>Si</td>\n",
       "      <td>Te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Darryl</td>\n",
       "      <td>It's not my real name. Darryl Philbin. Then Re...</td>\n",
       "      <td>ISTP</td>\n",
       "      <td>Ti</td>\n",
       "      <td>Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dwight</td>\n",
       "      <td>[singing] Shall I play for you? Pa rum pump um...</td>\n",
       "      <td>ESTJ</td>\n",
       "      <td>Te</td>\n",
       "      <td>Si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Holly</td>\n",
       "      <td>Hi. Yes, uh, I'm Holly Flax. I was told to ask...</td>\n",
       "      <td>INFP</td>\n",
       "      <td>Fi</td>\n",
       "      <td>Ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Jan</td>\n",
       "      <td>[on her cell phone] Just before lunch. That wo...</td>\n",
       "      <td>ENTJ</td>\n",
       "      <td>Te</td>\n",
       "      <td>Ni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jim</td>\n",
       "      <td>Oh, I told you. I couldn't close it. So... Act...</td>\n",
       "      <td>ENTP</td>\n",
       "      <td>Ne</td>\n",
       "      <td>Ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Karen</td>\n",
       "      <td>Jim's nice enough.  I dont... I don't know how...</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>Si</td>\n",
       "      <td>Te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Kelly</td>\n",
       "      <td>I have a customer meeting. I just had the long...</td>\n",
       "      <td>ESFP</td>\n",
       "      <td>Se</td>\n",
       "      <td>Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kevin</td>\n",
       "      <td>Yeah, it'll be you. I don't wanna be laid off....</td>\n",
       "      <td>ISFP</td>\n",
       "      <td>Fi</td>\n",
       "      <td>Se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Meredith</td>\n",
       "      <td>Michael. Michael, please, can we talk to you a...</td>\n",
       "      <td>ESTP</td>\n",
       "      <td>Se</td>\n",
       "      <td>Ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Michael</td>\n",
       "      <td>All right Jim. Your quarterlies look very good...</td>\n",
       "      <td>ENFP</td>\n",
       "      <td>Ne</td>\n",
       "      <td>Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Oscar</td>\n",
       "      <td>You guys better update your resumes just like ...</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>Ni</td>\n",
       "      <td>Te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Pam</td>\n",
       "      <td>Well. I don't know. What? Uh, yeah. Just a fax...</td>\n",
       "      <td>ISFJ</td>\n",
       "      <td>Si</td>\n",
       "      <td>Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Phyllis</td>\n",
       "      <td>So what does downsizing actually mean? What? W...</td>\n",
       "      <td>ISFJ</td>\n",
       "      <td>Si</td>\n",
       "      <td>Fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Roy</td>\n",
       "      <td>Hey, man. Hey, baby. No, no. Come on. Let's ge...</td>\n",
       "      <td>ESFP</td>\n",
       "      <td>Se</td>\n",
       "      <td>Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ryan</td>\n",
       "      <td>Hey. Ryan Howard from the temp agency. Daniqua...</td>\n",
       "      <td>ESTP</td>\n",
       "      <td>Se</td>\n",
       "      <td>Ti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Stanley</td>\n",
       "      <td>Well... It could be out of your hands Michael....</td>\n",
       "      <td>ISTJ</td>\n",
       "      <td>Si</td>\n",
       "      <td>Te</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Toby</td>\n",
       "      <td>Hey, we're not all going to sit in a circle In...</td>\n",
       "      <td>ISFJ</td>\n",
       "      <td>Si</td>\n",
       "      <td>Fe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     speaker                                          line_text  MBTI  F1  F2\n",
       "0       Andy  Hey, Big Tuna!  You're single right? She's pre...  ESFP  Se  Fi\n",
       "1     Angela  I bet it's gonna be me. Probably gonna be me. ...  ISTJ  Si  Te\n",
       "2     Darryl  It's not my real name. Darryl Philbin. Then Re...  ISTP  Ti  Se\n",
       "3     Dwight  [singing] Shall I play for you? Pa rum pump um...  ESTJ  Te  Si\n",
       "4      Holly  Hi. Yes, uh, I'm Holly Flax. I was told to ask...  INFP  Fi  Ne\n",
       "5        Jan  [on her cell phone] Just before lunch. That wo...  ENTJ  Te  Ni\n",
       "6        Jim  Oh, I told you. I couldn't close it. So... Act...  ENTP  Ne  Ti\n",
       "7      Karen  Jim's nice enough.  I dont... I don't know how...  ISTJ  Si  Te\n",
       "8      Kelly  I have a customer meeting. I just had the long...  ESFP  Se  Fi\n",
       "9      Kevin  Yeah, it'll be you. I don't wanna be laid off....  ISFP  Fi  Se\n",
       "10  Meredith  Michael. Michael, please, can we talk to you a...  ESTP  Se  Ti\n",
       "11   Michael  All right Jim. Your quarterlies look very good...  ENFP  Ne  Fi\n",
       "12     Oscar  You guys better update your resumes just like ...  INTJ  Ni  Te\n",
       "13       Pam  Well. I don't know. What? Uh, yeah. Just a fax...  ISFJ  Si  Fe\n",
       "14   Phyllis  So what does downsizing actually mean? What? W...  ISFJ  Si  Fe\n",
       "15       Roy  Hey, man. Hey, baby. No, no. Come on. Let's ge...  ESFP  Se  Fi\n",
       "16      Ryan  Hey. Ryan Howard from the temp agency. Daniqua...  ESTP  Se  Ti\n",
       "17   Stanley  Well... It could be out of your hands Michael....  ISTJ  Si  Te\n",
       "18      Toby  Hey, we're not all going to sit in a circle In...  ISFJ  Si  Fe"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Input/raw_agg.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c972d1",
   "metadata": {},
   "source": [
    "# 1. NLTK Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f55be2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vader = df.copy()\n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to apply sentiment analysis and extract sentiment scores\n",
    "def vader_pos(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['pos']\n",
    "def vader_neg(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['neg']\n",
    "def vader_neu(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['neu']\n",
    "def vader_comp(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    return scores['compound']\n",
    "\n",
    "\n",
    "# Apply sentiment analysis and create a new column with sentiment scores\n",
    "df_vader['vader_pos'] = df_vader['line_text'].apply(lambda text: vader_pos(text))\n",
    "df_vader['vader_neg'] = df_vader['line_text'].apply(lambda text: vader_neg(text))\n",
    "df_vader['vader_neu'] = df_vader['line_text'].apply(lambda text: vader_neu(text))\n",
    "#compound = (pos - neg) / (pos + neg + neu)\n",
    "df_vader['vader_comp'] = df_vader['line_text'].apply(lambda text: vader_comp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb4df519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a column \"comp_power\"\n",
    "df_vader['vader_power'] = (df_vader['vader_pos'] + df_vader['vader_neg']) \\\n",
    "                        / (df_vader['vader_pos'] + df_vader['vader_neg'] + df_vader['vader_neu'])\n",
    "df_vader.to_csv('Output/vader_scores.csv', index=False)\n",
    "#df_vader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b66f12",
   "metadata": {},
   "source": [
    "# 2. Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a12a7316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_blob = df.copy()\n",
    "\n",
    "# Apply sentiment analysis using TextBlob\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Apply the function and expand the result into separate columns\n",
    "df_blob[['polarity', 'subjectivity']] = df_blob['line_text'].apply(lambda x: analyze_sentiment(x)).apply(pd.Series)\n",
    "\n",
    "df_blob.to_csv('Output/textblob.csv', index=False)\n",
    "\n",
    "# Display the DataFrame with sentiment analysis results\n",
    "#df_blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb2b1ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>line_text</th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love this sunny weather, but I hate the heat.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.392998</td>\n",
       "      <td>0.607002</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This movie is amazing!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982438</td>\n",
       "      <td>0.017562</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am not sure about this product.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770051</td>\n",
       "      <td>0.229949</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The service was terrible.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.927603</td>\n",
       "      <td>0.072397</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         line_text  pos       neg       neu  \\\n",
       "0  I love this sunny weather, but I hate the heat.  0.0  0.392998  0.607002   \n",
       "1                           This movie is amazing!  0.0  0.982438  0.017562   \n",
       "2                I am not sure about this product.  0.0  0.770051  0.229949   \n",
       "3                        The service was terrible.  0.0  0.927603  0.072397   \n",
       "\n",
       "   polarity  subjectivity  \n",
       "0     -0.15      0.750000  \n",
       "1      0.75      0.900000  \n",
       "2     -0.25      0.888889  \n",
       "3     -1.00      1.000000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e69cae",
   "metadata": {},
   "source": [
    "# 3. Hugging face transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36bdc2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) negative 0.7236\n",
      "2) neutral 0.2287\n",
      "3) positive 0.0477\n"
     ]
    }
   ],
   "source": [
    "##Test\n",
    "\n",
    "# Preprocess text (username and link placeholders)\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "#model.save_pretrained(MODEL)\n",
    "text = \"Covid cases are increasing fast!\"\n",
    "text = preprocess(text)\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "scores = output[0][0].detach().numpy()\n",
    "scores = softmax(scores)\n",
    "# # TF\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "# model.save_pretrained(MODEL)\n",
    "# text = \"Covid cases are increasing fast!\"\n",
    "# encoded_input = tokenizer(text, return_tensors='tf')\n",
    "# output = model(encoded_input)\n",
    "# scores = output[0][0].numpy()\n",
    "# scores = softmax(scores)\n",
    "# Print labels and scores\n",
    "ranking = np.argsort(scores)\n",
    "ranking = ranking[::-1]\n",
    "for i in range(scores.shape[0]):\n",
    "    l = config.id2label[ranking[i]]\n",
    "    s = scores[ranking[i]]\n",
    "    print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3404ecf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (19474) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 19474].  Tensor sizes: [1, 514]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Apply sentiment analysis to each row in 'text' column\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m sentiment_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdf_roberta\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mline_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcalculate_sentiment_scores\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Extract scores and assign to new columns\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df_roberta[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(sentiment_scores, index\u001b[38;5;241m=\u001b[39mdf_roberta\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36mcalculate_sentiment_scores\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     20\u001b[0m text \u001b[38;5;241m=\u001b[39m preprocess(text)\n\u001b[0;32m     21\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\n\u001b[0;32m     23\u001b[0m scores \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     24\u001b[0m scores \u001b[38;5;241m=\u001b[39m softmax(scores)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1196\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1196\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1207\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1208\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:810\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    809\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[1;32m--> 810\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (19474) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 19474].  Tensor sizes: [1, 514]"
     ]
    }
   ],
   "source": [
    "df_roberta = df.copy()\n",
    "\n",
    "# Load tokenizer and model\n",
    "MODEL = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "\n",
    "#Preprocess\n",
    "def preprocess(text):\n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "#Apply\n",
    "def calculate_sentiment_scores(text):\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores.tolist()\n",
    "\n",
    "# Apply sentiment analysis to each row in 'text' column\n",
    "sentiment_scores = df_roberta['line_text'].apply(calculate_sentiment_scores).tolist()\n",
    "\n",
    "# Extract scores and assign to new columns\n",
    "df_roberta[['neg', 'neu', 'pos']] = pd.DataFrame(sentiment_scores, index=df_roberta.index)\n",
    "\n",
    "# Print the DataFrame with sentiment scores\n",
    "print(df_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b656d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929951c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
